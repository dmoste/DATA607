---
title: "DATA 607 Final Project"
author: "David Moste"
date: "5/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Introduction

For this project, I wanted to take on a task at the school where I work that has been of great interest. The reason I'm in this program (besides finding the material fun and interesting) is so that I can use some of these skills to help my school improve. To that end, I wanted to build a simple predictor that would be able to predict student scores in classes.  

At my school, student take courses from a local community college during their high school years. We have had great difficulty in deciding when to recommend students for particular college courses, so my goal with this project was to add in another data point to consider during the recommendation process: what grade do we think they'll receive.  

### Process

The first step was to load the required libraries and the datasets in their base form. There were quite a few libraries that were needed for this project!
```{r}
library(ggplot2)
library(tidyverse)
library(ranger)
library(missRanger)
library(data.table)
library(abind)
library(mongolite)
```

The first dataset was loaded in at the same time as some initial cleaning was performed. The initial cleaning here just separated course code data into useable columns and removed some unneeded columns. I also created a column to indicate if a course was a college course or not (many of the students at my school take college courses).
```{r}
set.seed(52638)

url <- "https://github.com/dmoste/DATA607/blob/master/Final/all_transcripts.csv?raw=true"
df <- read.csv(url, header = TRUE) %>%
  separate(Table, c("Code","GPA.Modify"), sep = "\\*") %>%
  select(-GPA.Modify,
         -Term.Average.Display,
         -Credits.Total,
         -Credit.Earned.Total,
         -Credits.Average,
         -Credits.Total.1,
         -Cumulative.Term.Average1,
         -Mark.1,
         -Credits.Average.1,
         -Credit.Earned.Total.1,
         -CreditEarned.1,
         -Mark,
         -CreditEarned) %>%
  mutate(College = ifelse(grepl("U$", Code) | Code == "HBS11UA", "Y", "N")) %>%
  tbl_df()
```

The second dataset was attendance data loaded in from MongoDB (my second data source). No cleaning was required for this set.
```{r}
my_collection <- mongo(collection = "attendance", db = "final")
attendance <- my_collection$find('{}')
```

Each row of the data is data on a single course taken by a student, where the rows are grouped by student. Unfortunately, StudentID is only listed in the last row of data for each student. I used nafill and next observation carried backwards to pull the StudentIDs up into each column for the students. I also created names that could be used as columns for the Code values so that each code could be used as feature by my predictor.
```{r}
df$StudentID <- nafill(df$StudentID, type = "nocb")
df$Code <- make.names(df$Code, unique = FALSE, allow_ = FALSE)
```

Here, I used pivot_wider to continue my cleaning efforts and create columns for each course so that courses could be used as features for prediction. I also joined my two datasets together at this step.
```{r}
df <- df %>%
  pivot_wider(names_from  = Code,
              values_from = NumericEquivalent) %>%
  group_by(StudentID) %>%
  left_join(attendance, by = "StudentID") %>%
  summarise_if(is.numeric,
               max,
               na.rm = TRUE)
```

Next, I filled all the -Inf created by my summarise_if statement with NAs. This is so I can impute this data in my next step. I also removed columns where more than 95% of students in the school had not taken the course. This removes some useless information from courses that won't be relevant predictors and will save time in the prediction phase.
```{r}
df[df == -Inf] <- NA
df <- df[, which(colMeans(!is.na(df)) > 0.05)]
df <- df %>%
  mutate(Imputed = ifelse(is.na(df$MGS22), 1, 0)) %>%
  select(-TermCD, -SchoolYear)
```

At this point, I have a bunch of NAs, which will cause major issues for my predictor. I used the missRanger package here to impute (predict) these values.
```{r}
df <- missRanger(df, num.trees = 100, maxiter = 5)
```

I've almost made it to my predictor! I just have to create a training dataset and a holdout dataset.
```{r}
sample_size = floor(0.75*nrow(df))
picked      = sample(seq_len(nrow(df)),
                     size = sample_size)
training    = df[picked,]
holdout     = df[-picked,]
```

Finally, I get to create my random forest predictor! The values I used for this came from a grid I setup on the side to determine which values would give me the lowest root mean squared error.
```{r}
OOB_RMSE <- vector(mode = "numeric", length = 100)

# Use the best parameter values to create 100 different models
for(i in seq_along(OOB_RMSE)) {
  optimal_ranger <- ranger(
    formula         = MGS22 ~ ., 
    data            = training, 
    num.trees       = 500,
    mtry            = 36,
    min.node.size   = 1,
    sample.fraction = .8,
    importance      = 'impurity')
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}
```

Next, I wanted to view the histogram showing RMSE just to make sure it was approximately normal.
```{r}
hist(OOB_RMSE, breaks = 20)
```

After confirmation, I loaded the importance variable into a dataframe and plotted so I could easily see waht the most important predictors were.
```{r}
importance <- data.frame(optimal_ranger$variable.importance)
setDT(importance, keep.rownames = TRUE)[]
names(importance) <- c("Variable", "Value")

importance %>%
  arrange(desc(Value)) %>%
  top_n(20) %>%
  ggplot(aes(reorder(Variable, Value), Value)) +
  geom_col() +
  coord_flip() +
  ggtitle("Top 20 important variables")
```

Now I wanted to run my predictor on my holdout dataset and plot the results.
```{r}
pred_ranger <- predict(optimal_ranger, holdout)
plot(pred_ranger[["predictions"]], holdout$MGS22)
```

Then I created a linear model to see how good my fit was.
```{r}
m_all <- lm(holdout$MGS22 ~ pred_ranger[["predictions"]])
summary(m_all)
plot(pred_ranger[["predictions"]], holdout$MGS22)
abline(m_all)
```

Looking quickly at the statistical output of my model, I stated the null and alternative hypotheses.  

$H_0:$ the true linear model has slope zero.  
$H_A:$ the true linear model has slope different than zero. The Ranger predictions are predictive of course scores. 

The equation created by the linear model:  

$\widehat{score}=-7.13+1.09\times prediction$  

A 95% confidence interval for this model:  

$CI=b\pm{t}^{*}_{df}\times SE_{b}$  
$CI=1.09\pm 1.979\times 0.03546$  
$CI=\left(1.02,1.16\right)$  

This confidence interval indicates that we are 95% confident that with each point increase in the prediction, the actual score is predicted to increase on average by 1.02 to 1.16 points.  

It's pretty clear from this analysis, that we can reject the null hypothesis.

Next, I circled back, knowing that I had imputed some values which may be assisting the quality of this model. I wanted to look at that breakdown and then remove the imputed values and create a new model.
```{r}
pf <-data.frame(holdout, pred_ranger[["predictions"]])
pf$Imputed <- as.factor(pf$Imputed)
ggplot(pf,
       aes(x     = pred_ranger...predictions...,
           y     = MGS22,
           color = Imputed)) + geom_point()

pf <- filter(pf, Imputed == 0)

m_real <- lm(pf$MGS22 ~ pf$pred_ranger...predictions...)
plot(pf$pred_ranger...predictions..., pf$MGS22)
abline(m_real)
summary(m_real)
```

Looking quickly at the statistical output of my model, I stated the null and alternative hypotheses.  

$H_0:$ the true linear model has slope zero.  
$H_A:$ the true linear model has slope different than zero. The Ranger predictions are predictive of course scores. 

The equation created by the linear model:  

$\widehat{score}=-12.9+1.166\times prediction$  

A 95% confidence interval for this model:  

$CI=b\pm{t}^{*}_{df}\times SE_{b}$  
$CI=1.166\pm 1.99\times 0.06075$  
$CI=\left(1.05,1.29\right)$  

This confidence interval indicates that we are 95% confident that with each point increase in the prediction, the actual score is predicted to increase on average by 1.05 to 1.29 points.  

It's pretty clear from this analysis, that we can reject the null hypothesis.

### Conclusion

This was a super fun project to work on, but I really wish I had MORE DATA POINTS! While this predictor does a decent job for courses with lots of data points, such as geometry (the course used in this example), the reliability obviously decreases significantly when you try it out on college courses, for which there is much less data. I'd love to keep the data I have and add to it each year so that I can build up a more robust predictor. I'd also like to add in professor data if possible, because I imagine that would make a difference as well.